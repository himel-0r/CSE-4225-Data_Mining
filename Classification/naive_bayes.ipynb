{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b799c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed1c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "ucirepo_ids = {\n",
    "    \"iris\": 53,\n",
    "    \"heart_disease\": 45,\n",
    "    \"molecular_biology\": 69,\n",
    "    \"breast_cancer\": 17,\n",
    "    \"adult\": 2,\n",
    "    \"bank_marketing\": 222,\n",
    "    \"student_performance\": 320,\n",
    "    \"wine\": 109,\n",
    "    \"air_quality\": 360,\n",
    "    \"mushroom\": 73\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"heart_disease\"  # Example dataset name\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "def custom_data():\n",
    "    \n",
    "    data = {\n",
    "            'age': ['youth', 'youth', 'middle aged', 'senior', 'senior', 'senior', 'middle aged',\n",
    "                    'youth', 'youth', 'senior', 'youth', 'middle aged', 'middle aged', 'senior'],\n",
    "            'income': ['high', 'high', 'high', 'medium', 'low', 'low', 'low',\n",
    "                    'medium', 'low', 'medium', 'medium', 'medium', 'high', 'medium'],\n",
    "            'student': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n",
    "                        'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no'],\n",
    "            'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent',\n",
    "                            'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'excellent'],\n",
    "            'buys_computer': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes',\n",
    "                            'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n",
    "        }\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Split into features and target\n",
    "    features = df.drop(columns='buys_computer')\n",
    "    targets = df['buys_computer']\n",
    "    targets= pd.DataFrame(targets.values.reshape(-1, 1), columns=['buys_computer'])\n",
    "\n",
    "    \n",
    "    # Variable info\n",
    "    variable_info = {\n",
    "        col: {\n",
    "            'type': 'categorical',\n",
    "            'unique_values': df[col].unique().tolist()\n",
    "        } for col in df.columns\n",
    "    }\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'source': 'Simulated AllElectronics dataset',\n",
    "        'description': 'Customer attributes and their decision to buy a computer',\n",
    "        'num_samples': len(df),\n",
    "        'num_features': features.shape[1],\n",
    "        'target_column': 'buys_computer',\n",
    "        'class_labels': sorted(df['buys_computer'].unique().tolist())\n",
    "    }\n",
    "\n",
    "    # Build nested structure\n",
    "    return SimpleNamespace(\n",
    "        data=SimpleNamespace(\n",
    "            features=features,\n",
    "            targets=targets,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            target_names=sorted(targets.iloc[:,0].unique()),\n",
    "            # frame=df\n",
    "        ),\n",
    "        metadata=metadata,\n",
    "        variables=variable_info\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_dataframe(dataframe_name):\n",
    "    \n",
    "    if dataframe_name == \"custom_data\":\n",
    "        df = custom_data()\n",
    "        \n",
    "        # metadata \n",
    "        print(df.metadata) \n",
    "        \n",
    "        # variable information \n",
    "        print(df.variables) \n",
    "    \n",
    "        return df\n",
    "    \n",
    "    if dataframe_name in ucirepo_ids:\n",
    "        # fetch dataset \n",
    "        df = fetch_ucirepo(id=ucirepo_ids[dataframe_name],) \n",
    "\n",
    "        # # data (as pandas dataframes) \n",
    "        X = df.data.features \n",
    "        y = df.data.targets \n",
    "        \n",
    "        # metadata \n",
    "        print(df.metadata) \n",
    "        \n",
    "        # variable information \n",
    "        print(df.variables) \n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataframe_name}' not found in UCI repository.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b066afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __train_test_split(X, y, test_size = 0.2, shuffle_and_stratify = True):\n",
    "    \n",
    "    if test_size < 0 or test_size > 1:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "   \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(\"Features and targets must have the same length.\")\n",
    "\n",
    "    \n",
    "    if shuffle_and_stratify == False:\n",
    "    \n",
    "        train_size = 1 - test_size\n",
    "        train_index = int(len(X) * train_size)\n",
    "        \n",
    "        X_train = X[0: train_index]\n",
    "        X_test = X[train_index:]\n",
    "        \n",
    "        y_train = y[0: train_index]\n",
    "        y_test = y[train_index:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        labels = y.iloc[:,0].unique()\n",
    "        X_train = pd.DataFrame(columns=X.columns)\n",
    "        y_train = pd.DataFrame(columns=y.columns)\n",
    "        X_test = pd.DataFrame(columns=X.columns)\n",
    "        y_test = pd.DataFrame(columns=y.columns)\n",
    "        \n",
    "        train_size = 1 - test_size\n",
    "        \n",
    "\n",
    "        for label in labels :\n",
    "            y_rows = y[y.iloc[:,0] == label]            \n",
    "            X_rows = X.loc[y_rows.index]\n",
    "            \n",
    "            train_index = int(len(X_rows) * train_size)\n",
    "            \n",
    "            X_train = pd.concat([X_train, X_rows.iloc[:train_index]], ignore_index=False)\n",
    "            y_train = pd.concat([y_train, y_rows.iloc[:train_index]] , ignore_index=False)\n",
    "            \n",
    "            X_test = pd.concat([X_test, X_rows[train_index:]], ignore_index=False)\n",
    "            y_test = pd.concat([y_test, y_rows[train_index:]], ignore_index=False)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77bcfedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 45, 'name': 'Heart Disease', 'repository_url': 'https://archive.ics.uci.edu/dataset/45/heart+disease', 'data_url': 'https://archive.ics.uci.edu/static/public/45/data.csv', 'abstract': '4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 303, 'num_features': 13, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': ['Age', 'Sex'], 'target_col': ['num'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1989, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C52P4X', 'creators': ['Andras Janosi', 'William Steinbrunn', 'Matthias Pfisterer', 'Robert Detrano'], 'intro_paper': {'ID': 231, 'type': 'NATIVE', 'title': 'International application of a new probability algorithm for the diagnosis of coronary artery disease.', 'authors': 'R. Detrano, A. JÃ¡nosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher', 'venue': 'American Journal of Cardiology', 'year': 1989, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/a7d714f8f87bfc41351eb5ae1e5472f0ebbe0574', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': '2756873', 'pmcid': None}, 'additional_info': {'summary': 'This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to date.  The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  \\n   \\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\\n\\nOne file has been \"processed\", that one containing the Cleveland database.  All four unprocessed files also exist in this directory.\\n\\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\" ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Only 14 attributes used:\\r\\n      1. #3  (age)       \\r\\n      2. #4  (sex)       \\r\\n      3. #9  (cp)        \\r\\n      4. #10 (trestbps)  \\r\\n      5. #12 (chol)      \\r\\n      6. #16 (fbs)       \\r\\n      7. #19 (restecg)   \\r\\n      8. #32 (thalach)   \\r\\n      9. #38 (exang)     \\r\\n      10. #40 (oldpeak)   \\r\\n      11. #41 (slope)     \\r\\n      12. #44 (ca)        \\r\\n      13. #51 (thal)      \\r\\n      14. #58 (num)       (the predicted attribute)\\r\\n\\r\\nComplete attribute documentation:\\r\\n      1 id: patient identification number\\r\\n      2 ccf: social security number (I replaced this with a dummy value of 0)\\r\\n      3 age: age in years\\r\\n      4 sex: sex (1 = male; 0 = female)\\r\\n      5 painloc: chest pain location (1 = substernal; 0 = otherwise)\\r\\n      6 painexer (1 = provoked by exertion; 0 = otherwise)\\r\\n      7 relrest (1 = relieved after rest; 0 = otherwise)\\r\\n      8 pncaden (sum of 5, 6, and 7)\\r\\n      9 cp: chest pain type\\r\\n        -- Value 1: typical angina\\r\\n        -- Value 2: atypical angina\\r\\n        -- Value 3: non-anginal pain\\r\\n        -- Value 4: asymptomatic\\r\\n     10 trestbps: resting blood pressure (in mm Hg on admission to the hospital)\\r\\n     11 htn\\r\\n     12 chol: serum cholestoral in mg/dl\\r\\n     13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker)\\r\\n     14 cigs (cigarettes per day)\\r\\n     15 years (number of years as a smoker)\\r\\n     16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\\r\\n     17 dm (1 = history of diabetes; 0 = no such history)\\r\\n     18 famhist: family history of coronary artery disease (1 = yes; 0 = no)\\r\\n     19 restecg: resting electrocardiographic results\\r\\n        -- Value 0: normal\\r\\n        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\\r\\n        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes\\' criteria\\r\\n     20 ekgmo (month of exercise ECG reading)\\r\\n     21 ekgday(day of exercise ECG reading)\\r\\n     22 ekgyr (year of exercise ECG reading)\\r\\n     23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no)\\r\\n     24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no)\\r\\n     26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no)\\r\\n     28 proto: exercise protocol\\r\\n          1 = Bruce     \\r\\n          2 = Kottus\\r\\n          3 = McHenry\\r\\n          4 = fast Balke\\r\\n          5 = Balke\\r\\n          6 = Noughton \\r\\n          7 = bike 150 kpa min/min  (Not sure if \"kpa min/min\" is what was written!)\\r\\n          8 = bike 125 kpa min/min  \\r\\n          9 = bike 100 kpa min/min\\r\\n         10 = bike 75 kpa min/min\\r\\n         11 = bike 50 kpa min/min\\r\\n         12 = arm ergometer\\r\\n     29 thaldur: duration of exercise test in minutes\\r\\n     30 thaltime: time when ST measure depression was noted\\r\\n     31 met: mets achieved\\r\\n     32 thalach: maximum heart rate achieved\\r\\n     33 thalrest: resting heart rate\\r\\n     34 tpeakbps: peak exercise blood pressure (first of 2 parts)\\r\\n     35 tpeakbpd: peak exercise blood pressure (second of 2 parts)\\r\\n     36 dummy\\r\\n     37 trestbpd: resting blood pressure\\r\\n     38 exang: exercise induced angina (1 = yes; 0 = no)\\r\\n     39 xhypo: (1 = yes; 0 = no)\\r\\n     40 oldpeak = ST depression induced by exercise relative to rest\\r\\n     41 slope: the slope of the peak exercise ST segment\\r\\n        -- Value 1: upsloping\\r\\n        -- Value 2: flat\\r\\n        -- Value 3: downsloping\\r\\n     42 rldv5: height at rest\\r\\n     43 rldv5e: height at peak exercise\\r\\n     44 ca: number of major vessels (0-3) colored by flourosopy\\r\\n     45 restckm: irrelevant\\r\\n     46 exerckm: irrelevant\\r\\n     47 restef: rest raidonuclid (sp?) ejection fraction\\r\\n     48 restwm: rest wall (sp?) motion abnormality\\r\\n        0 = none\\r\\n        1 = mild or moderate\\r\\n        2 = moderate or severe\\r\\n        3 = akinesis or dyskmem (sp?)\\r\\n     49 exeref: exercise radinalid (sp?) ejection fraction\\r\\n     50 exerwm: exercise wall (sp?) motion \\r\\n     51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\\r\\n     52 thalsev: not used\\r\\n     53 thalpul: not used\\r\\n     54 earlobe: not used\\r\\n     55 cmo: month of cardiac cath (sp?)  (perhaps \"call\")\\r\\n     56 cday: day of cardiac cath (sp?)\\r\\n     57 cyr: year of cardiac cath (sp?)\\r\\n     58 num: diagnosis of heart disease (angiographic disease status)\\r\\n        -- Value 0: < 50% diameter narrowing\\r\\n        -- Value 1: > 50% diameter narrowing\\r\\n        (in any major vessel: attributes 59 through 68 are vessels)\\r\\n     59 lmt\\r\\n     60 ladprox\\r\\n     61 laddist\\r\\n     62 diag\\r\\n     63 cxmain\\r\\n     64 ramus\\r\\n     65 om1\\r\\n     66 om2\\r\\n     67 rcaprox\\r\\n     68 rcadist\\r\\n     69 lvx1: not used\\r\\n     70 lvx2: not used\\r\\n     71 lvx3: not used\\r\\n     72 lvx4: not used\\r\\n     73 lvf: not used\\r\\n     74 cathef: not used\\r\\n     75 junk: not used\\r\\n     76 name: last name of patient  (I replaced this with the dummy string \"name\")', 'citation': None}}\n",
      "        name     role         type demographic  \\\n",
      "0        age  Feature      Integer         Age   \n",
      "1        sex  Feature  Categorical         Sex   \n",
      "2         cp  Feature  Categorical        None   \n",
      "3   trestbps  Feature      Integer        None   \n",
      "4       chol  Feature      Integer        None   \n",
      "5        fbs  Feature  Categorical        None   \n",
      "6    restecg  Feature  Categorical        None   \n",
      "7    thalach  Feature      Integer        None   \n",
      "8      exang  Feature  Categorical        None   \n",
      "9    oldpeak  Feature      Integer        None   \n",
      "10     slope  Feature  Categorical        None   \n",
      "11        ca  Feature      Integer        None   \n",
      "12      thal  Feature  Categorical        None   \n",
      "13       num   Target      Integer        None   \n",
      "\n",
      "                                          description  units missing_values  \n",
      "0                                                None  years             no  \n",
      "1                                                None   None             no  \n",
      "2                                                None   None             no  \n",
      "3   resting blood pressure (on admission to the ho...  mm Hg             no  \n",
      "4                                   serum cholestoral  mg/dl             no  \n",
      "5                     fasting blood sugar > 120 mg/dl   None             no  \n",
      "6                                                None   None             no  \n",
      "7                         maximum heart rate achieved   None             no  \n",
      "8                             exercise induced angina   None             no  \n",
      "9   ST depression induced by exercise relative to ...   None             no  \n",
      "10                                               None   None             no  \n",
      "11  number of major vessels (0-3) colored by flour...   None            yes  \n",
      "12                                               None   None            yes  \n",
      "13                         diagnosis of heart disease   None             no  \n"
     ]
    }
   ],
   "source": [
    "df = fetch_dataframe(DATASET_NAME)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c421e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (241, 13)\n",
      "X_test shape: (62, 13)\n",
      "y_train shape: (241, 1)\n",
      "y_test shape: (62, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_27232\\1895835202.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  X_train = pd.concat([X_train, X_rows.iloc[:train_index]], ignore_index=False)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_27232\\1895835202.py:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  X_test = pd.concat([X_test, X_rows[train_index:]], ignore_index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.data.features\n",
    "y = df.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = __train_test_split(X, y , test_size=TEST_SIZE, shuffle_and_stratify=True)\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e595ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>165</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>145</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows Ã 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex cp trestbps chol fbs restecg thalach exang  oldpeak slope   ca  \\\n",
       "0    63   1  1      145  233   1       2     150     0      2.3     3  0.0   \n",
       "3    37   1  3      130  250   0       0     187     0      3.5     3  0.0   \n",
       "4    41   0  2      130  204   0       2     172     0      1.4     1  0.0   \n",
       "5    56   1  2      120  236   0       0     178     0      0.8     1  0.0   \n",
       "7    57   0  4      120  354   0       0     163     1      0.6     1  0.0   \n",
       "..   ..  .. ..      ...  ...  ..     ...     ...   ...      ...   ...  ...   \n",
       "136  70   1  4      145  174   0       0     125     1      2.6     3  0.0   \n",
       "146  57   1  4      165  289   1       2     124     0      1.0     2  3.0   \n",
       "153  55   1  4      160  289   0       2     145     1      0.8     2  1.0   \n",
       "161  77   1  4      125  304   0       2     162     1      0.0     1  3.0   \n",
       "174  64   1  4      145  212   0       2     132     0      2.0     2  2.0   \n",
       "\n",
       "     thal  \n",
       "0     6.0  \n",
       "3     3.0  \n",
       "4     3.0  \n",
       "5     3.0  \n",
       "7     3.0  \n",
       "..    ...  \n",
       "136   7.0  \n",
       "146   7.0  \n",
       "153   7.0  \n",
       "161   3.0  \n",
       "174   6.0  \n",
       "\n",
       "[241 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e98417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num\n",
       "0     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "7     0\n",
       "..   ..\n",
       "136   4\n",
       "146   4\n",
       "153   4\n",
       "161   4\n",
       "174   4\n",
       "\n",
       "[241 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = y_train.iloc[:,0].unique()\n",
    "\n",
    "# for label in labels:\n",
    "    \n",
    "#     label_rows = y_train[y_train.iloc[:,0] == label]\n",
    "    \n",
    "#     row_indices = label_rows.index\n",
    "    \n",
    "#     print(row_indices)\n",
    "    \n",
    "#     corresponding_rows = X_train.loc[label_rows.index]\n",
    "    \n",
    "#     for ind in row_indices:\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if ind not in X_train.index:\n",
    "#             print(f\"{ind} not in X_train for label {label}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4ea5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, sqrt, pi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ALPHA = 1e-3\n",
    "\n",
    "\n",
    "def _is_continous(X: pd.Series) -> bool:\n",
    "    return np.issubdtype(X.dtype, np.number) and (len(X.unique()) / len(X) > 0.001)\n",
    "\n",
    "\n",
    "def gaussian(x_k, mean, std):\n",
    "    if std < 1e-9:\n",
    "        return 1.0 if abs(x_k - mean) < 1e-9 else 1e-9\n",
    "    coeff = 1 / (sqrt(2 * pi) * std)\n",
    "    exponent = exp(-((x_k - mean) ** 2) / (2 * std ** 2))\n",
    "    return coeff * exponent\n",
    "\n",
    "\n",
    "def naive_bayes_classifier_preds(X_train, y_train, X_test):\n",
    "    y_preds = []\n",
    "    attributes = X_train.columns\n",
    "    labels = y_train.iloc[:, 0].unique()\n",
    "\n",
    "    label_probs = {}\n",
    "    cond_probs = {}\n",
    "    mean_store = {}\n",
    "    std_store = {}\n",
    "\n",
    "    # Calculate prior probabilities\n",
    "    total_rows = len(y_train)\n",
    "    for label in labels:\n",
    "        count = (y_train.iloc[:, 0] == label).sum()\n",
    "        label_probs[label] = (count + ALPHA) / (total_rows + ALPHA * len(labels))\n",
    "\n",
    "        label_indices = y_train[y_train.iloc[:, 0] == label].index\n",
    "        label_X = X_train.loc[label_indices]\n",
    "\n",
    "        for attr in attributes:\n",
    "            if _is_continous(X_train[attr]):\n",
    "                mean = label_X[attr].mean()\n",
    "                std = max(label_X[attr].std(ddof=0), 1e-3)\n",
    "                mean_store[(attr, label)] = mean\n",
    "                std_store[(attr, label)] = std\n",
    "            else:\n",
    "                value_counts = label_X[attr].value_counts()\n",
    "                total = len(label_X)\n",
    "                for val, count in value_counts.items():\n",
    "                    cond_probs[(attr, val, label)] = (count + ALPHA) / (total + ALPHA * len(value_counts))\n",
    "\n",
    "    # Predict\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        class_probs = {}\n",
    "        for label in labels:\n",
    "            prob = label_probs[label]\n",
    "            for attr in attributes:\n",
    "                val = test_row[attr]\n",
    "                if _is_continous(X_train[attr]):\n",
    "                    mean = mean_store.get((attr, label), 0.0)\n",
    "                    std = std_store.get((attr, label), 1e-3)\n",
    "                    prob *= gaussian(val, mean, std)\n",
    "                else:\n",
    "                    prob *= cond_probs.get((attr, val, label), ALPHA)\n",
    "            class_probs[label] = prob\n",
    "\n",
    "        # Choose label with highest posterior probability\n",
    "        max_label = max(class_probs, key=class_probs.get)\n",
    "        y_preds.append(str(max_label))\n",
    "\n",
    "    print(\"Predictions Length:\", len(y_preds))\n",
    "    return y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bdc56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Length: 62\n"
     ]
    }
   ],
   "source": [
    "y_preds = naive_bayes_classifier_preds(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c871b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __classification_report(y_true, y_pred):\n",
    "    # Ensure y_true is a Series\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.iloc[:, 0]\n",
    "\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"prediction does not have same number of tuples as the true value set\")\n",
    "    \n",
    "    labels = pd.Series(y_true).unique()\n",
    "    \n",
    "    for label in labels:\n",
    "        P = N = TP = FP = TN = FN = 0\n",
    "        \n",
    "        for i in range(len(y_pred)):\n",
    "            true_label = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "            pred_label = y_pred[i]\n",
    "            \n",
    "            if true_label == label:\n",
    "                P += 1\n",
    "            else:\n",
    "                N += 1\n",
    "                \n",
    "            if true_label == label and pred_label == label:\n",
    "                TP += 1\n",
    "            elif true_label == label and pred_label != label:\n",
    "                FN += 1\n",
    "            elif true_label != label and pred_label == label:\n",
    "                FP += 1\n",
    "            elif true_label != label and pred_label != label:\n",
    "                TN += 1\n",
    "        \n",
    "        accuracy = (TP + TN) / (P + N) if (P + N) > 0 else 0.0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        sensitivity = TP / P if P > 0 else 0.0\n",
    "        specificity = TN / N if N > 0 else 0.0\n",
    "        support = P + N\n",
    "\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"  Accuracy   : {accuracy:.2f}\")\n",
    "        print(f\"  Precision  : {precision:.2f}\")\n",
    "        print(f\"  Recall     : {recall:.2f}\")\n",
    "        print(f\"  F1 Score   : {f1_score:.2f}\")\n",
    "        print(f\"  Sensitivity: {sensitivity:.2f}\")\n",
    "        print(f\"  Specificity: {specificity:.2f}\")\n",
    "        print(f\"  Support    : {support}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "    #overall accuracy\n",
    "    \n",
    "    mathced = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        true_label = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "        pred_label = y_pred[i]\n",
    "        \n",
    "        if true_label == pred_label:\n",
    "            mathced += 1\n",
    "    \n",
    "    overall_accuracy = mathced / len(y_pred) if len(y_pred) > 0 else 0.0\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0335ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.81        33\n",
      "           1       0.15      0.18      0.17        11\n",
      "           2       0.20      0.12      0.15         8\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.53        62\n",
      "   macro avg       0.26      0.27      0.26        62\n",
      "weighted avg       0.47      0.53      0.50        62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert to string labels (optional)\n",
    "y_test = [str(y) for y in y_test]\n",
    "y_preds = [str(y) for y in y_preds]\n",
    "\n",
    "# Generate report\n",
    "report = classification_report(y_test, y_preds)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20b59076",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nCategoricalNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m model = CategoricalNB()  \u001b[38;5;66;03m# Use CategoricalNB for categorical data\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# model = GaussianNB()  # Use GaussianNB for numerical data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m y_preds_sklearn = model.predict(X_test_sk)\n\u001b[32m     25\u001b[39m report_sklearn = classification_report(y_test_sk, y_preds_sklearn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1388\u001b[39m, in \u001b[36mCategoricalNB.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1364\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[32m   1365\u001b[39m \n\u001b[32m   1366\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1386\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m   1387\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:735\u001b[39m, in \u001b[36m_BaseDiscreteNB.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    716\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[32m    717\u001b[39m \n\u001b[32m    718\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     X, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m     _, n_features = X.shape\n\u001b[32m    738\u001b[39m     labelbin = LabelBinarizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1454\u001b[39m, in \u001b[36mCategoricalNB._check_X_y\u001b[39m\u001b[34m(self, X, y, reset)\u001b[39m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1463\u001b[39m     check_non_negative(X, \u001b[33m\"\u001b[39m\u001b[33mCategoricalNB (input X)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1044\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1042\u001b[39m     array = _asarray_with_order(array, order=order, xp=xp)\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xp.isdtype(array.dtype, (\u001b[33m\"\u001b[39m\u001b[33mreal floating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomplex floating\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nCategoricalNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "X_sk = df.data.features\n",
    "y_sk = df.data.targets\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_encoded = encoder.fit_transform(X_sk)\n",
    "\n",
    "# print(f\"X_encoded {X_encoded}\")\n",
    "\n",
    "X_train_sk, X_test_sk, y_train_sk, y_test_sk = train_test_split(X_encoded, y_sk , test_size=0.2, random_state=42)\n",
    "\n",
    "# print(\"X_train\", X_train_sk)\n",
    "\n",
    "model = CategoricalNB()  # Use CategoricalNB for categorical data\n",
    "# model = GaussianNB()  # Use GaussianNB for numerical data\n",
    "\n",
    "model.fit(X_train_sk, y_train_sk)\n",
    "\n",
    "y_preds_sklearn = model.predict(X_test_sk)\n",
    "\n",
    "report_sklearn = classification_report(y_test_sk, y_preds_sklearn)\n",
    "\n",
    "print(\"Classification Report (sklearn):\")\n",
    "print(report_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d9de785",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds_sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m y_test = pd.DataFrame(y_test_sk.values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), columns=[\u001b[33m'\u001b[39m\u001b[33mbuys_computer\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create a comparison list\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m ys = [[y_test.iloc[i, \u001b[32m0\u001b[39m], y_preds[i], \u001b[43my_preds_sklearn\u001b[49m[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test))]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print comparison\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComparison of Predictions:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_preds_sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert y_test_sk to a DataFrame\n",
    "y_test = pd.DataFrame(y_test_sk.values.reshape(-1, 1), columns=['buys_computer'])\n",
    "\n",
    "# Create a comparison list\n",
    "ys = [[y_test.iloc[i, 0], y_preds[i], y_preds_sklearn[i]] for i in range(len(y_test))]\n",
    "\n",
    "# Print comparison\n",
    "print(\"Comparison of Predictions:\")\n",
    "for i, (actual, naive_bayes_pred, sklearn_pred) in enumerate(ys):\n",
    "    print(f\"Sample {i+1}: Actual: {actual}, Naive Bayes: {naive_bayes_pred}, sklearn: {sklearn_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df227c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
