{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7763caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109553ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "ucirepo_ids = {\n",
    "    \"iris\": 53,\n",
    "    \"heart_disease\": 45,\n",
    "    \"molecular_biology\": 69,\n",
    "    \"breast_cancer\": 17,\n",
    "    \"adult\": 2,\n",
    "    \"bank_marketing\": 222,\n",
    "    \"student_performance\": 320,\n",
    "    \"wine\": 109,\n",
    "    \"air_quality\": 360,\n",
    "    \"mushroom\": 73\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"adult\"  # Example dataset name\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "def custom_data():\n",
    "    \n",
    "    data = {\n",
    "            'age': ['youth', 'youth', 'middle aged', 'senior', 'senior', 'senior', 'middle aged',\n",
    "                    'youth', 'youth', 'senior', 'youth', 'middle aged', 'middle aged', 'senior'],\n",
    "            'income': ['high', 'high', 'high', 'medium', 'low', 'low', 'low',\n",
    "                    'medium', 'low', 'medium', 'medium', 'medium', 'high', 'medium'],\n",
    "            'student': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n",
    "                        'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no'],\n",
    "            'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent',\n",
    "                            'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'excellent'],\n",
    "            'buys_computer': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes',\n",
    "                            'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n",
    "        }\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Split into features and target\n",
    "    features = df.drop(columns='buys_computer')\n",
    "    targets = df['buys_computer']\n",
    "    targets= pd.DataFrame(targets.values.reshape(-1, 1), columns=['buys_computer'])\n",
    "\n",
    "    \n",
    "    # Variable info\n",
    "    variable_info = {\n",
    "        col: {\n",
    "            'type': 'categorical',\n",
    "            'unique_values': df[col].unique().tolist()\n",
    "        } for col in df.columns\n",
    "    }\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'source': 'Simulated AllElectronics dataset',\n",
    "        'description': 'Customer attributes and their decision to buy a computer',\n",
    "        'num_samples': len(df),\n",
    "        'num_features': features.shape[1],\n",
    "        'target_column': 'buys_computer',\n",
    "        'class_labels': sorted(df['buys_computer'].unique().tolist())\n",
    "    }\n",
    "\n",
    "    # Build nested structure\n",
    "    return SimpleNamespace(\n",
    "        data=SimpleNamespace(\n",
    "            features=features,\n",
    "            targets=targets,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            target_names=sorted(targets.iloc[:,0].unique()),\n",
    "            # frame=df\n",
    "        ),\n",
    "        metadata=metadata,\n",
    "        variables=variable_info\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_dataframe(dataframe_name):\n",
    "    \n",
    "    if dataframe_name == \"custom_data\":\n",
    "        df = custom_data()\n",
    "        \n",
    "        # metadata \n",
    "        print(df.metadata) \n",
    "        \n",
    "        # variable information \n",
    "        print(df.variables) \n",
    "    \n",
    "        return df\n",
    "    \n",
    "    if dataframe_name in ucirepo_ids:\n",
    "        # fetch dataset \n",
    "        df = fetch_ucirepo(id=ucirepo_ids[dataframe_name],) \n",
    "\n",
    "        # # data (as pandas dataframes) \n",
    "        X = df.data.features \n",
    "        y = df.data.targets \n",
    "        \n",
    "        # metadata \n",
    "        print(df.metadata) \n",
    "        \n",
    "        # variable information \n",
    "        print(df.variables) \n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataframe_name}' not found in UCI repository.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __train_test_split(X, y, test_size = 0.2, shuffle_and_stratify = True):\n",
    "    \n",
    "    if test_size < 0 or test_size > 1:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "   \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(\"Features and targets must have the same length.\")\n",
    "\n",
    "    \n",
    "    if shuffle_and_stratify == False:\n",
    "    \n",
    "        train_size = 1 - test_size\n",
    "        train_index = int(len(X) * train_size)\n",
    "        \n",
    "        X_train = X[0: train_index]\n",
    "        X_test = X[train_index:]\n",
    "        \n",
    "        y_train = y[0: train_index]\n",
    "        y_test = y[train_index:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        labels = y.iloc[:,0].unique()\n",
    "        X_train = pd.DataFrame(columns=X.columns)\n",
    "        y_train = pd.DataFrame(columns=y.columns)\n",
    "        X_test = pd.DataFrame(columns=X.columns)\n",
    "        y_test = pd.DataFrame(columns=y.columns)\n",
    "        \n",
    "        train_size = 1 - test_size\n",
    "        \n",
    "\n",
    "        for label in labels :\n",
    "            y_rows = y[y.iloc[:,0] == label]            \n",
    "            X_rows = X.loc[y_rows.index]\n",
    "            \n",
    "            train_index = int(len(X_rows) * train_size)\n",
    "            \n",
    "            X_train = pd.concat([X_train, X_rows.iloc[:train_index]], ignore_index=False)\n",
    "            y_train = pd.concat([y_train, y_rows.iloc[:train_index]] , ignore_index=False)\n",
    "            \n",
    "            X_test = pd.concat([X_test, X_rows[train_index:]], ignore_index=False)\n",
    "            y_test = pd.concat([y_test, y_rows[train_index:]], ignore_index=False)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "3563c141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "df = fetch_dataframe(DATASET_NAME)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "dff2a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39072, 14)\n",
      "X_test shape: (9770, 14)\n",
      "y_train shape: (39072, 1)\n",
      "y_test shape: (9770, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.data.features\n",
    "y = df.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = __train_test_split(X, y , test_size=TEST_SIZE, shuffle_and_stratify=True)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "21eac845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __classification_report(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.iloc[:, 0]\n",
    "\n",
    "    # Ensure y_true is a Series\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "        y_pred = y_pred.reset_index(drop=True)\n",
    "    elif isinstance(y_pred, list):\n",
    "        y_pred = pd.Series(y_pred)\n",
    "\n",
    "\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"prediction does not have same number of tuples as the true value set\")\n",
    "    \n",
    "    labels = pd.Series(y_true).unique()\n",
    "    \n",
    "    for label in labels:\n",
    "        P = N = TP = FP = TN = FN = 0\n",
    "        \n",
    "        for i in range(len(y_pred)):\n",
    "            true_label = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "            pred_label = y_pred.iloc[i] if hasattr(y_pred, 'iloc') else y_pred[i]\n",
    "            \n",
    "            if true_label == label:\n",
    "                P += 1\n",
    "            else:\n",
    "                N += 1\n",
    "                \n",
    "            if true_label == label and pred_label == label:\n",
    "                TP += 1\n",
    "            elif true_label == label and pred_label != label:\n",
    "                FN += 1\n",
    "            elif true_label != label and pred_label == label:\n",
    "                FP += 1\n",
    "            elif true_label != label and pred_label != label:\n",
    "                TN += 1\n",
    "        \n",
    "        accuracy = (TP + TN) / (P + N) if (P + N) > 0 else 0.0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        sensitivity = TP / P if P > 0 else 0.0\n",
    "        specificity = TN / N if N > 0 else 0.0\n",
    "        support = P\n",
    "\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"  Accuracy   : {accuracy:.2f}\")\n",
    "        print(f\"  Precision  : {precision:.2f}\")\n",
    "        print(f\"  Recall     : {recall:.2f}\")\n",
    "        print(f\"  F1 Score   : {f1_score:.2f}\")\n",
    "        print(f\"  Sensitivity: {sensitivity:.2f}\")\n",
    "        print(f\"  Specificity: {specificity:.2f}\")\n",
    "        print(f\"  Support    : {support}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "    #overall accuracy\n",
    "    \n",
    "    mathced = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        true_label = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "        pred_label = y_pred.iloc[i] if hasattr(y_pred, 'iloc') else y_pred[i]\n",
    "        \n",
    "        if true_label == pred_label:\n",
    "            mathced += 1\n",
    "    \n",
    "    overall_accuracy = mathced / len(y_pred) if len(y_pred) > 0 else 0.0\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "48447c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _is_continous(X: pd.Series) -> bool:\n",
    "    return np.issubdtype(X.dtype, np.number) and (len(X.unique()) / len(X) > 0.001)\n",
    "\n",
    "        \n",
    "\n",
    "class DiscreteAttributeSelectionCriteria:\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = value \n",
    "        \n",
    "    def condition_satisfied(self, val):\n",
    "        if self.value == None:\n",
    "            raise ValueError(f\"value not set\")\n",
    "        \n",
    "        if self.value == val:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        \n",
    "class ContinuousAtrributeSelectionCriteria:\n",
    "    \n",
    "    def __init__(self, start_point, end_point):\n",
    "        self.start_point = start_point\n",
    "        self.end_point = end_point\n",
    "    \n",
    "    def condition_satisfied(self, val):\n",
    "        if self.start_point == None or self.end_point==None:\n",
    "            raise ValueError(f\"value not set\")\n",
    "        \n",
    "        if self.start_point <= val <= self.end_point:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "class DiscreteAndBinaryAtrributeSelectionCriteria:\n",
    "    \n",
    "    def __init__(self, set_of_values):\n",
    "        self.value_set = set_of_values\n",
    "\n",
    "    def condition_satisfied(self, val):\n",
    "        if len(self.value_set) == 0:\n",
    "            raise ValueError(f\"value not set\")\n",
    "        \n",
    "        if val in self.value_set:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "\n",
    "def get_criteria(D,\n",
    "                best_splitting_criterion,\n",
    "                splitting_attr,\n",
    "                multiple_splits_allowed:bool=True\n",
    "                ):\n",
    "    \n",
    "    \n",
    "    if not _is_continous(D[splitting_attr]):\n",
    "        \n",
    "        if multiple_splits_allowed:\n",
    "            return DiscreteAttributeSelectionCriteria(value=best_splitting_criterion.value)\n",
    "        else:\n",
    "            return DiscreteAndBinaryAtrributeSelectionCriteria(set_of_values=best_splitting_criterion.set)\n",
    "    else:\n",
    "        return ContinuousAtrributeSelectionCriteria(start_point=best_splitting_criterion.start_point, \n",
    "                                                    end_point=best_splitting_criterion.end_point)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f4fecfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "\n",
    "def info(D):\n",
    "    label_counts = D.iloc[:, -1].value_counts().to_dict()\n",
    "    info_val = 0\n",
    "    for label in label_counts:\n",
    "        pi = label_counts[label] / len(D)\n",
    "        info_val += - pi * log2(pi)\n",
    "    return info_val\n",
    "\n",
    "def info_A(D, attr):\n",
    "    info_A = 0\n",
    "    attr_values = D[attr].unique()\n",
    "    for attr_val in attr_values:\n",
    "        Dj = D[D[attr] == attr_val]\n",
    "        info_A += (len(Dj) / len(D)) * info(Dj)\n",
    "    return info_A\n",
    "\n",
    "def Gain(D, A):\n",
    "    return info(D) - info_A(D, A)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "39134172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from platform import node\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.isLeaf = False\n",
    "        self.split_attribute = None  # <-- add this\n",
    "        self.returning_class = None\n",
    "        self.attribute_selection_criteria = None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, multiple_splits_allowed=True):\n",
    "        self.root = None\n",
    "        self.multiple_splits_allowed = multiple_splits_allowed\n",
    "\n",
    "    def attribute_selection(self, D, attribute_list):\n",
    "        best_gain = -1\n",
    "        best_attr = None\n",
    "        best_criterion = None\n",
    "\n",
    "        for attr in attribute_list:\n",
    "            \n",
    "            if len(D[attr].unique()) <= 1:\n",
    "                continue\n",
    "\n",
    "            if _is_continous(D[attr]):\n",
    "                sorted_vals = np.sort(D[attr].dropna().unique())\n",
    "                split_points = [(sorted_vals[i] + sorted_vals[i+1]) / 2 for i in range(len(sorted_vals)-1)]\n",
    "\n",
    "                for split in split_points:\n",
    "                    D_left = D[D[attr] <= split]\n",
    "                    D_right = D[D[attr] > split]\n",
    "                    if len(D_left) == 0 or len(D_right) == 0:\n",
    "                        continue\n",
    "                    weighted_info = (len(D_left)/len(D)) * info(D_left) + (len(D_right)/len(D)) * info(D_right)\n",
    "                    gain = info(D) - weighted_info\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_attr = attr\n",
    "                        best_criterion = [split]  # Store best split point\n",
    "            else:\n",
    "                gain = Gain(D, attr)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_attr = attr\n",
    "                    best_criterion = D[attr].unique()\n",
    "\n",
    "        return best_criterion, best_attr\n",
    "\n",
    "    def build_tree(self, X_train, y_train):\n",
    "        # Combine features and labels into one DataFrame\n",
    "        D = pd.concat([X_train, y_train], axis=1)\n",
    "        attribute_list = set(X_train.columns)\n",
    "\n",
    "        def generate_decision_tree(D, attribute_list):\n",
    "            node = Node()\n",
    "\n",
    "            # Stopping condition 1: All samples have the same label\n",
    "            if len(D.iloc[:, -1].unique()) == 1:\n",
    "                node.isLeaf = True\n",
    "                node.returning_class = D.iloc[:, -1].iloc[0]\n",
    "                return node\n",
    "\n",
    "            # Stopping condition 2: No attributes left to split\n",
    "            if len(attribute_list) == 0:\n",
    "                node.isLeaf = True\n",
    "                majority_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                node.returning_class = majority_class\n",
    "                return node\n",
    "\n",
    "            best_criterion, best_attr = self.attribute_selection(D, attribute_list)\n",
    "            \n",
    "            node.split_attribute = best_attr  # Store the attribute name\n",
    "            \n",
    "            if not self.multiple_splits_allowed:\n",
    "                attribute_list = attribute_list - {best_attr}\n",
    "\n",
    "            # If no attribute gives positive gain, make leaf node with majority class\n",
    "            if best_attr is None or len(best_criterion) == 0:\n",
    "                node.isLeaf = True\n",
    "                node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                return node\n",
    "\n",
    "            \n",
    "            if _is_continous(D[best_attr]):\n",
    "                split = best_criterion[0]\n",
    "                \n",
    "                node.attribute_selection_criteria = ContinuousAtrributeSelectionCriteria(start_point=-float('inf'), end_point=split)\n",
    "                \n",
    "                D_left = D[D[best_attr] <= split]\n",
    "                D_right = D[D[best_attr] > split]\n",
    "                \n",
    "                if len(D_left) == 0 or len(D_right) == 0:\n",
    "                    node.isLeaf = True\n",
    "                    node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                    return node\n",
    "\n",
    "\n",
    "                node.children['left'] = generate_decision_tree(D_left, attribute_list.copy())\n",
    "                node.children['right'] = generate_decision_tree(D_right, attribute_list.copy())\n",
    "            else:\n",
    "                node.attribute_selection_criteria = DiscreteAttributeSelectionCriteria(value=best_criterion[0])\n",
    "            # If multiple splits are NOT allowed, remove the chosen attribute\n",
    "                \n",
    "\n",
    "                # Split dataset by each attribute value and recurse\n",
    "                for attr_val in best_criterion:\n",
    "                    D_j = D[D[best_attr] == attr_val]\n",
    "\n",
    "                    # If no samples in this subset, create leaf with majority class of parent\n",
    "                    if len(D_j) == 0:\n",
    "                        leaf_node = Node()\n",
    "                        leaf_node.isLeaf = True\n",
    "                        leaf_node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                        node.children[attr_val] = leaf_node\n",
    "                    else:\n",
    "                        node.children[attr_val] = generate_decision_tree(D_j, attribute_list.copy())\n",
    "\n",
    "            return node\n",
    "\n",
    "        self.root = generate_decision_tree(D, attribute_list)\n",
    "\n",
    "    def _majority_class(self, node):\n",
    "        from collections import Counter\n",
    "\n",
    "        def collect_leaf_classes(n):\n",
    "            if n.isLeaf:\n",
    "                return [n.returning_class]\n",
    "            labels = []\n",
    "            for child in n.children.values():\n",
    "                labels.extend(collect_leaf_classes(child))\n",
    "            return labels\n",
    "\n",
    "        leaf_classes = collect_leaf_classes(node)\n",
    "        if not leaf_classes:\n",
    "            return None\n",
    "        return Counter(leaf_classes).most_common(1)[0][0]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        node = self.root\n",
    "        while not node.isLeaf:\n",
    "            attr = node.split_attribute  # <--- Get column name\n",
    "            val = x[attr]                # <--- Safe: x[attr] is now x[\"age\"] or similar\n",
    "\n",
    "            if isinstance(node.attribute_selection_criteria, ContinuousAtrributeSelectionCriteria):\n",
    "                if node.attribute_selection_criteria.condition_satisfied(val):\n",
    "                    node = node.children['left']\n",
    "                else:\n",
    "                    node = node.children['right']\n",
    "            elif isinstance(node.attribute_selection_criteria, DiscreteAttributeSelectionCriteria):\n",
    "                if node.attribute_selection_criteria.condition_satisfied(val):\n",
    "                    node = node.children[val]\n",
    "                else:\n",
    "                    return self._majority_class(node)\n",
    "\n",
    "        return node.returning_class\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self.predict_single(row) for _, row in X_test.iterrows()]\n",
    "        return pd.Series(predictions, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6f29cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.build_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "57e6c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example for prediction: {'age': 28, 'workclass': 'Private', 'fnlwgt': 198258, 'education': 'Assoc-voc', 'education-num': 11, 'marital-status': 'Married-civ-spouse', 'occupation': 'Sales', 'relationship': 'Husband', 'race': 'White', 'sex': 'Male', 'capital-gain': 0, 'capital-loss': 0, 'hours-per-week': 40, 'native-country': 'United-States'}\n",
      "Predicted class: <=50K -- expected: <=50K)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example prediction\n",
    "example = X_test.iloc[0]\n",
    "print(f\"Example for prediction: {example.to_dict()}\")\n",
    "predicted_class = model.predict_single(example)\n",
    "print(f\"Predicted class: {predicted_class} -- expected: {y_test.iloc[0, 0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "cb0e8315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test type: <class 'pandas.core.frame.DataFrame'>\n",
      "y_test shape: (9770, 1)\n",
      "y_preds type: <class 'pandas.core.series.Series'>\n",
      "y_preds shape: (9770,)\n",
      "Label: <=50K\n",
      "  Accuracy   : 0.51\n",
      "  Precision  : 0.51\n",
      "  Recall     : 1.00\n",
      "  F1 Score   : 0.67\n",
      "  Sensitivity: 1.00\n",
      "  Specificity: 0.00\n",
      "  Support    : 4944\n",
      "------------------------------\n",
      "Label: >50K\n",
      "  Accuracy   : 0.84\n",
      "  Precision  : 0.00\n",
      "  Recall     : 0.00\n",
      "  F1 Score   : 0.00\n",
      "  Sensitivity: 0.00\n",
      "  Specificity: 1.00\n",
      "  Support    : 1569\n",
      "------------------------------\n",
      "Label: <=50K.\n",
      "  Accuracy   : 0.75\n",
      "  Precision  : 0.00\n",
      "  Recall     : 0.00\n",
      "  F1 Score   : 0.00\n",
      "  Sensitivity: 0.00\n",
      "  Specificity: 1.00\n",
      "  Support    : 2487\n",
      "------------------------------\n",
      "Label: >50K.\n",
      "  Accuracy   : 0.92\n",
      "  Precision  : 0.00\n",
      "  Recall     : 0.00\n",
      "  F1 Score   : 0.00\n",
      "  Sensitivity: 0.00\n",
      "  Specificity: 1.00\n",
      "  Support    : 770\n",
      "------------------------------\n",
      "Overall Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test)\n",
    "\n",
    "print(\"y_test type:\", type(y_test))\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_preds type:\", type(y_preds))\n",
    "print(\"y_preds shape:\", y_preds.shape)\n",
    "\n",
    "if len(y_test) != len(y_preds):\n",
    "    raise ValueError(\"y_test and y_preds must have the same length.\")\n",
    "# Evaluate the model\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_preds)}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_preds))\n",
    "\n",
    "\n",
    "__classification_report(y_test, y_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
